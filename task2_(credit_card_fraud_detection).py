# -*- coding: utf-8 -*-
"""Task2_(Credit_Card_Fraud_Detection).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BvZ79yyyTxSF_KeKts-af8_73QVnGTMp
"""

import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
from sklearn.ensemble import RandomForestClassifier
#from sklearn.preprocessing import StandardScalar
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report

cd=pd.read_csv('creditcard.csv')

cd.head(6)

cd.describe()

cd.shape
#cd.dtypes

missing_values=cd.isnull().sum()
#missing_values

#c1=cd['Class'].value.counts()
#cd.drop('Class',axis='columns',inplace=True)

#cd1=cd[cd.V23!=' ']
#cd1=cd[cd.V24!='']
#cd1=cd[cd.V25!='']
#cd1=cd[cd.V26!='']
##cd1=cd[cd.V27!='']
#cd1=cd[cd.V28!='']
#cd1=cd[cd.Amount!='']
#cd1=cd[cd.Class!='']

missing_values1=cd.isnull().sum()
missing_values1
cd.shape

cd.fillna(0)

most_fre=cd.V28.value_counts()
most_fre

cd.fillna(most_fre)

fraud_count=cd["Class"].value_counts()
fraud_rate=100*fraud_count/cd.shape[0]
fraud_data=pd.concat([fraud_count,fraud_rate],axis=1).reset_index()
fraud_data.colums=['Class','Count','Percentage']
fraud_data

#imbalance to balance
cd_fraud=cd[cd['Class']==1]
cd_not_fraud=cd[cd['Class']==0]
cd_not_fraud_sampled=cd_not_fraud.sample(cd_fraud.shape[0],replace=False,random_state=101)
cd_balanced=pd.concat([cd_not_fraud_sampled,cd_fraud],axis=0).sample(frac=1,replace=False,random_state=101).reset_index().drop('index',axis=1)
cd_balanced

fraud_count=cd_balanced["Class"].value_counts()
fraud_rate=100*fraud_count/cd.shape[0]
fraud_data=pd.concat([fraud_count,fraud_rate],axis=1).reset_index()
fraud_data.colums=['Class','Count','Percentage']
fraud_data

from sklearn.model_selection import train_test_split

x_train,x_test,y_train,y_test = train_test_split(cd_balanced.drop('Class',axis=1),cd_balanced['Class'],test_size=0.2,random_state=90)
print(f'''x_train: {x_train.shape}
x_test: {x_test.shape}
y_train: {y_train.shape}
y_test: {y_test.shape}''' )

pip install preprocessing

from sklearn.preprocessing import StandardScaler

#model
RandomForestModel = Pipeline([
    ('scalar', StandardScaler()),
    ('classifier', RandomForestClassifier())
])
RandomForestModel.fit(x_train,y_train)

y_pre=RandomForestModel.predict(x_test)
y_pre

cr1=classification_report(y_test,y_pre)
print(cr1)

